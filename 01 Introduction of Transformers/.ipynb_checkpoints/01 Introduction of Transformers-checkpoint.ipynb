{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58373dd5-47fe-4ca6-a3a0-975d1394ba60",
   "metadata": {},
   "source": [
    "# <center>Introduction of Transformers</center>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea687397-f446-4397-b4f8-ca4d74b653bb",
   "metadata": {},
   "source": [
    "> **`I just want to drive the car, I don't care how it works.`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98767379-5a91-4ce2-9a35-19759b07d640",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a538413f-ce6f-4642-acde-38d0bf22a04c",
   "metadata": {},
   "source": [
    "Transformers are a type of neural network that can process sequential data, such as text, speech, or images, without relying on recurrent or convolutional layers. Instead, they use a mechanism called **attention** to learn the relationships between different elements in the input and output sequences. Attention allows the model to focus on the most relevant parts of the input for each element of the output, and to encode the context and position of each element in the sequence.\n",
    "\n",
    "Transformers consist of two main components: an **encoder** and a **decoder**. The encoder takes the input sequence and transforms it into a high-dimensional representation, called the **hidden state**. The decoder takes the hidden state and generates the output sequence, one element at a time. Both the encoder and the decoder are composed of multiple identical layers, each containing two sub-layers: a **multi-head attention** layer and a **feed-forward** layer. The multi-head attention layer allows the model to attend to different parts of the sequence simultaneously, using multiple attention heads. The feed-forward layer applies a non-linear transformation to the output of the attention layer.\n",
    "\n",
    "Transformers have been shown to achieve state-of-the-art results in various natural language processing tasks, such as machine translation, text summarization, question answering, and natural language generation. Some of the most famous Transformer models are **BERT**, **GPT-2**, and **GPT-3**. BERT is a bidirectional encoder that can learn from both left and right context, and can be fine-tuned for various downstream tasks. GPT-2 and GPT-3 are large-scale generative models that can produce coherent and diverse text on various topics, given a prompt or a context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63f1e35-fa25-4b3f-8726-9e625b034e14",
   "metadata": {},
   "source": [
    "### Transformer's main points\n",
    "1. The **attention mechanism** helps the Neural Network to learn from sequences that are very long.\n",
    "    * They can learn from longer sequences than LSTM, which is another type of RNNs.\n",
    "    * Attention was made for LSTM, but Transformers use only attention, and do not use LSTM at all.\n",
    "2. Transformers are big and slow\n",
    "    * But computations can be done in parallel (unlike RNNs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c49fbba-5d2c-4742-a6a2-3bb2830e1dd1",
   "metadata": {},
   "source": [
    "There isn't just one type of transformer, there are many:\n",
    "* BERT\n",
    "* GPT\n",
    "\n",
    "> Transformers can be applied to anything."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62f9859-7d43-43b8-8b93-93660407b24a",
   "metadata": {},
   "source": [
    "## Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea91b0b-203b-4aee-b2fb-5c21352984d2",
   "metadata": {},
   "source": [
    "* Sentiment Analysis\n",
    "* Embeddings and nearest neighbour search\n",
    "* Named Entity Recognition (many to many)\n",
    "* Text generation\n",
    "* Masked Language Model\n",
    "* Text summarization (sequence to sequence)\n",
    "* Language translation (used for building intuition for Attention)\n",
    "* Question Answering\n",
    "* Zero-Shot classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc79a16-822e-42c5-8140-b2a07ee7bdfc",
   "metadata": {},
   "source": [
    "## How we get from RNNs to Transformers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657fc459-6923-4216-92b1-b2ff701afe65",
   "metadata": {},
   "source": [
    "RNNs and Transformers are two different types of neural networks that can handle sequential data, such as text, speech, or images. However, they have different ways of processing and learning from the data.\n",
    "\n",
    "**RNNs** are sequential models that process data one element at a time, maintaining an internal hidden state that is updated at each step. They operate in a recurrent manner, where the output at each step depends on the previous hidden state and the current input. This allows them to capture the temporal dependencies and context in the data. However, RNNs have some limitations, such as:\n",
    "- They are slow to train, as they cannot be parallelized due to their sequential nature.\n",
    "- They suffer from the vanishing or exploding gradient problem, where the influence of earlier inputs diminishes or grows exponentially as the sequence progresses, making it difficult to capture long-term dependencies.\n",
    "- They have a fixed-length representation of the input sequence, which may lose some information or introduce noise.\n",
    "\n",
    "**Transformers** are non-sequential models that process data in parallel, using a mechanism called attention to learn the relationships between different elements in the input and output sequences. They do not rely on recurrent or convolutional layers, but instead use multiple layers of self-attention and cross-attention to encode and decode the data. This allows them to capture the global dependencies and context in the data. Some of the advantages of Transformers are:\n",
    "- They are fast to train, as they can be parallelized and distributed across multiple devices.\n",
    "- They do not suffer from the vanishing or exploding gradient problem, as they do not have recurrent connections or backpropagation through time.\n",
    "- They have a variable-length representation of the input sequence, which can preserve more information and reduce noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e24fb88-4296-4dab-99d7-b62b288b530f",
   "metadata": {},
   "source": [
    "## Sentiment Analysis (Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a4e519-37a9-4103-a80a-b8ef19930bb2",
   "metadata": {},
   "source": [
    "**Sentiment analysis** is a NLP technique that detects and extracts the subjective information in a text, such as the author's attitude, opinion, emotion, or sentiment. Some applications of Sentiment analysis are:\n",
    "\n",
    "- **Reputation management**: Monitoring social media platforms and analyzing the feedback and opinions of customers on products, services, or brands.\n",
    "\n",
    "- **Competitive intelligence**: Comparing the sentiment of a business and its competitors, and identifying their strengths, weaknesses, opportunities, and threats.\n",
    "\n",
    "- **Customer support**: Determining the best response based on the customer's sentiment, and providing personalized and empathetic solutions.\n",
    "\n",
    "- **Stock trading**: Reading news articles/tweets that might affect the stock prices of companies, and using sentiment to decide whether to buy or sell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b91a2d-e0a1-4e2c-ba8e-d4185a705921",
   "metadata": {},
   "source": [
    "### Why use Transformers instead of Bag of Words (BOW) model.\n",
    "\n",
    "- Transformers can capture the **meaning and context** of the text, while bag of words only counts the **frequency** of the words.\n",
    "- Transformers can handle **long and complex** sequences of words, while bag of words suffers from the **sparsity and dimensionality** problems.\n",
    "- Transformers can learn from **pretrained language models**, while bag of words requires **manual feature engineering** or **domain knowledge**.\n",
    "- Transformers can perform **self-attention** and **cross-attention**, while bag of words ignores the **order and position** of the words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc38d929-6867-4faf-87b1-f8b61ea67a3f",
   "metadata": {},
   "source": [
    "### Hugging Face Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296fd59c-d196-4f29-af8d-cc2f78b033f4",
   "metadata": {},
   "source": [
    "Hugging Face **pipeline** is a simple and powerful way to use pre-trained models for various natural language processing tasks, such as text classification, sentiment analysis, question answering, and more.\n",
    "\n",
    "- It is a **wrapper** around the Hugging Face Transformers library, which provides a large collection of state-of-the-art models for natural language understanding and generation.\n",
    "\n",
    "- It offers a simple and unified API that **abstracts** away the complex code and logic behind the models, and allows users to focus on the input and output of the task.\n",
    "\n",
    "- It supports **multiple tasks**, such as named entity recognition, masked language modeling, sentiment analysis, feature extraction, and question answering. Each task has a dedicated pipeline class that inherits from the base pipeline class.\n",
    "\n",
    "- It allows users to **easily load and use** any model from the Hugging Face Hub, which is a platform that hosts thousands of models from the community and the Hugging Face team.\n",
    "\n",
    "- It provides various options and parameters to customize the pipeline behavior, such as the device (CPU or GPU), the batch size, the return type (dict or list), the top-k results, the aggregation strategy, and more..\n",
    "\n",
    "#### Workflow\n",
    "* **Import** the pipeline\n",
    "* **Load** a pretrained model\n",
    "* **Use** the pre-trained model (simply pass the string/list of strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df99d852-2b38-48be-9c7c-336b80c98a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998340606689453}]\n",
      "[{'label': 'NEGATIVE', 'score': 0.9995469450950623}]\n",
      "[{'label': 'POSITIVE', 'score': 0.9998340606689453}, {'label': 'NEGATIVE', 'score': 0.9995469450950623}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create your pipeline (includes tokenization, etc...)\n",
    "classifier = pipeline(\n",
    "    task = \"sentiment-analysis\",\n",
    "    model = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "\n",
    "# No need to convert inputs into PyTorch Tensors, Numpy array, Tensorflow Tensor, etc.\n",
    "# We can simply pass the raw text to the model without any preprocessing.\n",
    "\n",
    "# Output is a dictionary\n",
    "print(classifier(\"This is such a great movie. It can be watched more than once.\"))  # passing single input\n",
    "print(classifier(\"This is not a great movie. It can't be watched more than once.\")) # passing single input\n",
    "\n",
    "print(classifier(\n",
    "    [\n",
    "        \"This is such a great movie. It can be watched more than once.\",\n",
    "        \"This is not a great movie. It can't be watched more than once.\"\n",
    "    ]\n",
    ")) # passing list of inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab19d40-f3c3-4b28-abdf-04eec6424056",
   "metadata": {},
   "source": [
    "## Sentiment Analysis in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "183f3579-468c-44b0-b52f-611bf4677095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "import sklearn.model_selection as model_selection\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e03a33d6-9c41-4a45-866e-976a7309ff8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/cuda/__init__.py:769\u001b[0m, in \u001b[0;36mcurrent_device\u001b[0;34m()\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcurrent_device\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    768\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the index of a currently selected device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 769\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    770\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_getDevice()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/cuda/__init__.py:289\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m     )\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    292\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    293\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "torch.cuda.is_available()\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "525468e3-6d9f-44de-909a-51545c34f875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.pipelines.text_classification.TextClassificationPipeline'>\n"
     ]
    }
   ],
   "source": [
    "# hugging face pipeline\n",
    "classifier = pipeline(\n",
    "    task = \"sentiment-analysis\",\n",
    "    model = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "\n",
    "print(type(classifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71c0a9a-6557-4735-af77-6cc0499670d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
