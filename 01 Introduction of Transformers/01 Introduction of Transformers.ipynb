{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58373dd5-47fe-4ca6-a3a0-975d1394ba60",
   "metadata": {},
   "source": [
    "# <center>Introduction of Transformers</center>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea687397-f446-4397-b4f8-ca4d74b653bb",
   "metadata": {},
   "source": [
    "> **`I just want to drive the car, I don't care how it works.`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98767379-5a91-4ce2-9a35-19759b07d640",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a538413f-ce6f-4642-acde-38d0bf22a04c",
   "metadata": {},
   "source": [
    "Transformers are a type of neural network that can process sequential data, such as text, speech, or images, without relying on recurrent or convolutional layers. Instead, they use a mechanism called **attention** to learn the relationships between different elements in the input and output sequences. Attention allows the model to focus on the most relevant parts of the input for each element of the output, and to encode the context and position of each element in the sequence.\n",
    "\n",
    "Transformers consist of two main components: an **encoder** and a **decoder**. The encoder takes the input sequence and transforms it into a high-dimensional representation, called the **hidden state**. The decoder takes the hidden state and generates the output sequence, one element at a time. Both the encoder and the decoder are composed of multiple identical layers, each containing two sub-layers: a **multi-head attention** layer and a **feed-forward** layer. The multi-head attention layer allows the model to attend to different parts of the sequence simultaneously, using multiple attention heads. The feed-forward layer applies a non-linear transformation to the output of the attention layer.\n",
    "\n",
    "Transformers have been shown to achieve state-of-the-art results in various natural language processing tasks, such as machine translation, text summarization, question answering, and natural language generation. Some of the most famous Transformer models are **BERT**, **GPT-2**, and **GPT-3**. BERT is a bidirectional encoder that can learn from both left and right context, and can be fine-tuned for various downstream tasks. GPT-2 and GPT-3 are large-scale generative models that can produce coherent and diverse text on various topics, given a prompt or a context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c49fbba-5d2c-4742-a6a2-3bb2830e1dd1",
   "metadata": {},
   "source": [
    "There isn't just one type of transformer, there are many:\n",
    "* BERT\n",
    "* GPT\n",
    "\n",
    "> Transformers can be applied to anything."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62f9859-7d43-43b8-8b93-93660407b24a",
   "metadata": {},
   "source": [
    "## Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea91b0b-203b-4aee-b2fb-5c21352984d2",
   "metadata": {},
   "source": [
    "* Sentiment Analysis\n",
    "* Embeddings and nearest neighbour search\n",
    "* Named Entity Recognition (many to many)\n",
    "* Text generation\n",
    "* Masked Language Model\n",
    "* Text summarization (sequence to sequence)\n",
    "* Language translation (used for building intuition for Attention)\n",
    "* Question Answering\n",
    "* Zero-Shot classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc79a16-822e-42c5-8140-b2a07ee7bdfc",
   "metadata": {},
   "source": [
    "## How we get from RNNs to Transformers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657fc459-6923-4216-92b1-b2ff701afe65",
   "metadata": {},
   "source": [
    "RNNs and Transformers are two different types of neural networks that can handle sequential data, such as text, speech, or images. However, they have different ways of processing and learning from the data. Here is a brief intuition of how we get from RNNs to Transformers:\n",
    "\n",
    "- RNNs are sequential models that process data one element at a time, maintaining an internal hidden state that is updated at each step. They operate in a recurrent manner, where the output at each step depends on the previous hidden state and the current input. This allows them to capture the temporal dependencies and context in the data. However, RNNs have some limitations, such as:\n",
    "    - They are slow to train, as they cannot be parallelized due to their sequential nature.\n",
    "    - They suffer from the vanishing or exploding gradient problem, where the influence of earlier inputs diminishes or grows exponentially as the sequence progresses, making it difficult to capture long-term dependencies.\n",
    "    - They have a fixed-length representation of the input sequence, which may lose some information or introduce noise.\n",
    "- Transformers are non-sequential models that process data in parallel, using a mechanism called attention to learn the relationships between different elements in the input and output sequences. They do not rely on recurrent or convolutional layers, but instead use multiple layers of self-attention and cross-attention to encode and decode the data. This allows them to capture the global dependencies and context in the data. Some of the advantages of Transformers are:\n",
    "    - They are fast to train, as they can be parallelized and distributed across multiple devices.\n",
    "    - They do not suffer from the vanishing or exploding gradient problem, as they do not have recurrent connections or backpropagation through time.\n",
    "    - They have a variable-length representation of the input sequence, which can preserve more information and reduce noise.\n",
    "\n",
    "To summarize, RNNs and Transformers are two different approaches to sequence modeling, with different strengths and weaknesses. RNNs are good at capturing local and sequential dependencies, but have problems with long-term dependencies and scalability. Transformers are good at capturing global and parallel dependencies, but have problems with redundancy and interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc363d8-715d-4a2d-aef5-039c7a39fe86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea35e49-c2ce-425f-8bea-efa6f955d842",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
